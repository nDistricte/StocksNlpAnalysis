{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizerFast\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>159.369995</td>\n",
       "      <td>161.050003</td>\n",
       "      <td>159.350006</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>160.120499</td>\n",
       "      <td>51305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5847</th>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>161.529999</td>\n",
       "      <td>162.470001</td>\n",
       "      <td>161.270004</td>\n",
       "      <td>162.360001</td>\n",
       "      <td>161.704056</td>\n",
       "      <td>49501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>162.440002</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>161.910004</td>\n",
       "      <td>164.899994</td>\n",
       "      <td>164.233795</td>\n",
       "      <td>68749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5849</th>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>164.270004</td>\n",
       "      <td>166.289993</td>\n",
       "      <td>164.220001</td>\n",
       "      <td>166.169998</td>\n",
       "      <td>165.498672</td>\n",
       "      <td>56976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5850</th>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>166.839996</td>\n",
       "      <td>165.110001</td>\n",
       "      <td>165.630005</td>\n",
       "      <td>164.960846</td>\n",
       "      <td>46278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>182.089996</td>\n",
       "      <td>185.600006</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>185.559998</td>\n",
       "      <td>185.559998</td>\n",
       "      <td>59144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>183.919998</td>\n",
       "      <td>185.149994</td>\n",
       "      <td>182.729996</td>\n",
       "      <td>185.139999</td>\n",
       "      <td>185.139999</td>\n",
       "      <td>42841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>184.350006</td>\n",
       "      <td>186.399994</td>\n",
       "      <td>183.919998</td>\n",
       "      <td>186.190002</td>\n",
       "      <td>186.190002</td>\n",
       "      <td>46792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>186.539993</td>\n",
       "      <td>187.050003</td>\n",
       "      <td>183.619995</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>49128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>186.059998</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>185.190002</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>40444700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "5846  2023-03-29  159.369995  161.050003  159.350006  160.770004  160.120499   \n",
       "5847  2023-03-30  161.529999  162.470001  161.270004  162.360001  161.704056   \n",
       "5848  2023-03-31  162.440002  165.000000  161.910004  164.899994  164.233795   \n",
       "5849  2023-04-03  164.270004  166.289993  164.220001  166.169998  165.498672   \n",
       "5850  2023-04-04  166.600006  166.839996  165.110001  165.630005  164.960846   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6041  2024-01-08  182.089996  185.600006  181.500000  185.559998  185.559998   \n",
       "6042  2024-01-09  183.919998  185.149994  182.729996  185.139999  185.139999   \n",
       "6043  2024-01-10  184.350006  186.399994  183.919998  186.190002  186.190002   \n",
       "6044  2024-01-11  186.539993  187.050003  183.619995  185.589996  185.589996   \n",
       "6045  2024-01-12  186.059998  186.740005  185.190002  185.919998  185.919998   \n",
       "\n",
       "        Volume  \n",
       "5846  51305700  \n",
       "5847  49501700  \n",
       "5848  68749800  \n",
       "5849  56976200  \n",
       "5850  46278300  \n",
       "...        ...  \n",
       "6041  59144500  \n",
       "6042  42841800  \n",
       "6043  46792900  \n",
       "6044  49128400  \n",
       "6045  40444700  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock = pd.read_csv(r\"C:/Users/Asatl/onedrive/desktop/let/AAPL.csv\")\n",
    "df_stock = df_stock[-200:]\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime format\n",
    "df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target variable 'Close' (dependent variable) from the DataFrame\n",
    "y = df_stock['Close']\n",
    "\n",
    "# Extracting the feature 'Open' (independent variable) from the DataFrame\n",
    "X = df_stock['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data as MLPRegressor expects a 2D array\n",
    "X = X.values.reshape(-1, 1)\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = ARIMA(y_train ,order=(5,1,0))\n",
    "arima_model_fit = arima_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "forecast_steps = len(y_test)  # Forecast for the length of the test set\n",
    "forecast = arima_model_fit.forecast(steps=forecast_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts for the T+1:\n",
      "[181.8143647]\n"
     ]
    }
   ],
   "source": [
    "# Forecast T+1\n",
    "forecast_steps = 1\n",
    "forecast_T1 = arima_model_fit.forecast(steps=forecast_steps)\n",
    "print(\"Forecasts for the T+1:\")\n",
    "print(forecast_T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts for the T+7:\n",
      "[181.8143647  180.01493013 182.41845257 183.23032458 182.07883239\n",
      " 181.42262658 181.77849107]\n"
     ]
    }
   ],
   "source": [
    "# Forecast T+7\n",
    "forecast_steps = 7\n",
    "forecast_T1 = arima_model_fit.forecast(steps=forecast_steps)\n",
    "print(\"Forecasts for the T+7:\")\n",
    "print(forecast_T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Root Mean Squared Error (RMSE): 9.984752410044385\n",
      "Mean Absolute Percentage Error (MAPE): 4.8608461227649045\n",
      "Mean Absolute Error (MAE): 8.795999586135926\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "mape_test = mean_absolute_percentage_error(y_test, forecast)\n",
    "mae_test = np.mean(np.abs(y_test - forecast))\n",
    "\n",
    "# Print metrics for test set\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_test}')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Excited to see what's next for $AAPL in the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>$AAPL's stock performance reflects its solid f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Wondering how $AAPL will capitalize on the lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>$AAPL's innovation engine never ceases to amaze.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Anticipating the impact of $AAPL's upcoming pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     No.                                            Message\n",
       "0    1.0  Excited to see what's next for $AAPL in the te...\n",
       "1    2.0  $AAPL's stock performance reflects its solid f...\n",
       "2    3.0  Wondering how $AAPL will capitalize on the lat...\n",
       "3    4.0   $AAPL's innovation engine never ceases to amaze.\n",
       "4    5.0  Anticipating the impact of $AAPL's upcoming pr...\n",
       "..   ...                                                ...\n",
       "201  NaN                                                NaN\n",
       "202  NaN                                                NaN\n",
       "203  NaN                                                NaN\n",
       "204  NaN                                                NaN\n",
       "205  NaN                                                NaN\n",
       "\n",
       "[206 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Tweets = pd.read_csv(r\"C:\\Users\\ASatl\\OneDrive\\Desktop\\let\\tweets.csv\")\n",
    "df_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in the 'Message' column\n",
    "df_Tweets = df_Tweets.dropna(subset=['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis using Word2Vec\n",
    "tweets_texts = df_Tweets['Message'].tolist()\n",
    "tweets_tokens = [tweet_text.split() for tweet_text in tweets_texts]\n",
    "Word2Vec_model = Word2Vec(tweets_tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained FinBERT model and tokenizer\n",
    "finbert_model = TFBertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "finbert_tokenizer = BertTokenizerFast.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize tweet texts using FinBERT tokenizer\n",
    "tokenized_tweets = finbert_tokenizer(tweets_texts, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "7/7 [==============================] - 13s 238ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtain predictions from FinBERT model\n",
    "finbert_predictions = finbert_model.predict(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentiment scores from predictions\n",
    "positive_scores = finbert_predictions[0][:, 0]  # Positive sentiment score\n",
    "negative_scores = finbert_predictions[0][:, 1]  # Negative sentiment score\n",
    "neutral_scores = finbert_predictions[0][:, 2]   # Neutral sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sentiment scores as additional features\n",
    "additional_features = np.column_stack((positive_scores, negative_scores, neutral_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional features with existing features (y_train, y_test)\n",
    "y_train_with_features = np.concatenate((y_train, additional_features[:len(y_train)]), axis=1)\n",
    "y_test_with_features = np.concatenate((y_test, additional_features[len(y_train):]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192.75    , 186.190002, 171.559998, 167.449997, 174.789993,\n",
       "       185.559998, 173.559998, 163.759995, 173.      , 192.580002,\n",
       "       176.300003, 193.419998, 163.759995, 170.690002, 175.009995,\n",
       "       190.690002, 175.839996, 174.490005, 189.429993, 176.570007,\n",
       "       189.369995, 174.      , 172.570007, 165.229996, 175.050003,\n",
       "       179.070007, 177.300003, 193.580002, 179.800003, 180.710007,\n",
       "       183.789993, 171.210007, 189.690002, 175.429993, 177.970001,\n",
       "       179.360001, 173.570007, 177.149994, 177.229996, 177.559998,\n",
       "       164.899994, 193.130005, 179.580002, 193.050003, 185.639999,\n",
       "       195.610001, 189.710007, 175.160004, 192.529999, 188.059998,\n",
       "       169.589996, 175.460007, 181.119995, 172.070007, 165.210007,\n",
       "       181.179993, 187.440002, 164.660004, 173.5     , 175.490005,\n",
       "       165.630005, 172.070007, 172.880005, 171.100006, 189.699997,\n",
       "       173.440002, 165.559998, 189.25    , 178.179993, 166.889999,\n",
       "       189.970001, 160.770004, 185.589996, 179.229996, 188.080002,\n",
       "       176.080002, 193.970001, 180.089996, 191.449997, 171.770004,\n",
       "       172.990005, 189.460007, 182.889999, 182.410004, 165.789993,\n",
       "       168.539993, 194.830002, 168.220001, 194.5     , 171.839996,\n",
       "       190.399994, 179.210007, 179.460007, 181.820007, 177.25    ,\n",
       "       178.720001, 166.169998, 184.119995, 183.949997, 178.990005,\n",
       "       198.110001, 194.710007, 194.679993, 180.570007, 193.619995,\n",
       "       172.690002, 162.029999, 182.910004, 178.190002, 195.830002,\n",
       "       195.710007, 197.570007, 178.850006, 160.800003, 166.470001,\n",
       "       186.679993, 191.240005, 174.910004, 165.330002, 190.539993,\n",
       "       194.270004, 178.389999, 189.789993, 196.940002, 189.589996,\n",
       "       186.009995, 187.869995, 180.960007, 185.139999, 187.      ,\n",
       "       177.820007, 181.990005, 169.679993, 183.960007, 188.009995,\n",
       "       184.25    , 172.399994, 174.199997, 186.399994, 181.910004,\n",
       "       162.360001, 183.309998, 170.770004, 173.660004, 177.570007,\n",
       "       178.610001, 175.839996, 175.740005, 191.169998, 193.990005,\n",
       "       173.929993, 185.919998, 168.410004, 193.149994, 189.770004,\n",
       "       187.649994, 167.630005, 177.970001, 197.960007, 176.380005])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_with_features[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SARIMAX models require univariate `endog`. Got shape (160, 4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m endog_variable \u001b[38;5;241m=\u001b[39m y_train_with_features\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create the ARIMA model using the univariate endogenous variable\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m arima_model_feature \u001b[38;5;241m=\u001b[39m \u001b[43mARIMA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog_variable\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fit the ARIMA model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m arima_model_fit_feature \u001b[38;5;241m=\u001b[39m arima_model_feature\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[1;32mc:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\statsmodels\\tsa\\arima\\model.py:158\u001b[0m, in \u001b[0;36mARIMA.__init__\u001b[1;34m(self, endog, exog, order, seasonal_order, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[0;32m    151\u001b[0m     trend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Construct the specification\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# (don't pass specific values of enforce stationarity/invertibility,\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# because we don't actually want to restrict the estimators based on\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# this criteria. Instead, we'll just make sure that the parameter\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# estimates from those methods satisfy the criteria.)\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_arima \u001b[38;5;241m=\u001b[39m \u001b[43mSARIMAXSpecification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_stationarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_invertibility\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcentrate_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcentrate_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_specification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_specification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m exog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_arima\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39morig_exog\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Raise an error if we have a constant in an integrated model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\statsmodels\\tsa\\arima\\specification.py:454\u001b[0m, in \u001b[0;36mSARIMAXSpecification.__init__\u001b[1;34m(self, endog, exog, order, seasonal_order, ar_order, diff, ma_order, seasonal_ar_order, seasonal_diff, seasonal_ma_order, seasonal_periods, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Validate endog shape\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (validate_specification \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m faux_endog \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSARIMAX models require univariate `endog`. Got\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    455\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_missing \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m faux_endog \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog)))\n",
      "\u001b[1;31mValueError\u001b[0m: SARIMAX models require univariate `endog`. Got shape (160, 4)."
     ]
    }
   ],
   "source": [
    "# Select only the first column (y_train) from y_train_with_features\n",
    "endog_variable = y_train_with_features\n",
    "# Create the ARIMA model using the univariate endogenous variable\n",
    "arima_model_feature = ARIMA(endog_variable ,order=(5,1,0))\n",
    "# Fit the ARIMA model\n",
    "arima_model_fit_feature = arima_model_feature.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "forecast_steps = len(y_test_with_features)  # Forecast for the length of the test set\n",
    "forecast_feature = arima_model_fit_feature.forecast(steps=forecast_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -162.95449705,  -110.05516624,  -200.932584  ,  -288.99702457,\n",
       "        -261.0201476 ,  -378.47416069,  -409.42046932,  -431.77540424,\n",
       "        -531.79127994,  -542.4946542 ,  -604.02317124,  -669.96075434,\n",
       "        -691.59976754,  -765.6076946 ,  -806.25424276,  -849.43603008,\n",
       "        -915.04814726,  -948.70080886, -1006.74539441, -1057.81047518,\n",
       "       -1097.80436406, -1158.40499115, -1200.15958676, -1249.60561527,\n",
       "       -1304.47836436, -1345.2463357 , -1399.95731156, -1447.82872958,\n",
       "       -1492.7162826 , -1547.00259901, -1591.19184282, -1640.50859062,\n",
       "       -1691.22223173, -1735.63765775, -1786.83253137, -1834.10668064,\n",
       "       -1880.68973747, -1931.09970086, -1976.83602994, -2025.29203572])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts for the T+1:\n",
      "[-162.95449705]\n"
     ]
    }
   ],
   "source": [
    "# Forecast T+1\n",
    "forecast_steps = 1\n",
    "forecast_T1_feature = arima_model_fit_feature.forecast(steps=forecast_steps)\n",
    "print(\"Forecasts for the T+1:\")\n",
    "print(forecast_T1_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts for the T+7:\n",
      "[-162.95449705 -110.05516624 -200.932584   -288.99702457 -261.0201476\n",
      " -378.47416069 -409.42046932]\n"
     ]
    }
   ],
   "source": [
    "# Forecast T+7\n",
    "forecast_steps = 7\n",
    "forecast_T1_feature = arima_model_fit_feature.forecast(steps=forecast_steps)\n",
    "print(\"Forecasts for the T+7:\")\n",
    "print(forecast_T1_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Root Mean Squared Error (RMSE): 1379.4601011531843\n",
      "Mean Absolute Percentage Error (MAPE): 690.0325898232641\n",
      "Mean Absolute Error (MAE): 1256.875327527775\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, forecast_feature))\n",
    "mape_test = mean_absolute_percentage_error(y_test, forecast_feature)\n",
    "mae_test = np.mean(np.abs(y_test - forecast_feature))\n",
    "\n",
    "# Print metrics for test set\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_test}')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
