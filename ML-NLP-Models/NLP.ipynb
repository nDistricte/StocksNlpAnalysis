{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Excited to see what's next for $AAPL in the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>$AAPL's stock performance reflects its solid f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wondering how $AAPL will capitalize on the lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>$AAPL's innovation engine never ceases to amaze.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Anticipating the impact of $AAPL's upcoming pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>184</td>\n",
       "      <td>Investors remain bullish on $AAPL despite regu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>185</td>\n",
       "      <td>The success of Apple Fitness+ highlights $AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>186</td>\n",
       "      <td>$AAPL's commitment to reducing its environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>187</td>\n",
       "      <td>Investors are excited about $AAPL's expansion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>188</td>\n",
       "      <td>The iPhone 17 launch is expected to drive stro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     No.                                            Message\n",
       "0      1  Excited to see what's next for $AAPL in the te...\n",
       "1      2  $AAPL's stock performance reflects its solid f...\n",
       "2      3  Wondering how $AAPL will capitalize on the lat...\n",
       "3      4   $AAPL's innovation engine never ceases to amaze.\n",
       "4      5  Anticipating the impact of $AAPL's upcoming pr...\n",
       "..   ...                                                ...\n",
       "183  184  Investors remain bullish on $AAPL despite regu...\n",
       "184  185  The success of Apple Fitness+ highlights $AAPL...\n",
       "185  186  $AAPL's commitment to reducing its environment...\n",
       "186  187  Investors are excited about $AAPL's expansion ...\n",
       "187  188  The iPhone 17 launch is expected to drive stro...\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ASatl\\OneDrive\\Desktop\\let\\tweets.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASatl\\.cache\\huggingface\\hub\\models--yiyanghkust--finbert-tone. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASatl\\OneDrive\\Desktop\\let\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained FinBERT model and tokenizer\n",
    "finbert_model = TFBertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "finbert_tokenizer = BertTokenizerFast.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tweet texts from the \"Message\" column\n",
    "tweet_texts = df[\"Message\"].tolist()\n",
    "\n",
    "# Tokenize each tweet text into individual words\n",
    "tweet_tokens = [tweet_text.split() for tweet_text in tweet_texts]\n",
    "\n",
    "# Train Word2Vec model on the tokenized tweet data\n",
    "word2vec_model = Word2Vec(tweet_tokens, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.936384</td>\n",
       "      <td>1.004464</td>\n",
       "      <td>0.907924</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.847207</td>\n",
       "      <td>535796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0.966518</td>\n",
       "      <td>0.987723</td>\n",
       "      <td>0.903460</td>\n",
       "      <td>0.915179</td>\n",
       "      <td>0.775779</td>\n",
       "      <td>512377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0.926339</td>\n",
       "      <td>0.987165</td>\n",
       "      <td>0.919643</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.787131</td>\n",
       "      <td>778321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>0.947545</td>\n",
       "      <td>0.955357</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.719014</td>\n",
       "      <td>767972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>0.861607</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.852679</td>\n",
       "      <td>0.888393</td>\n",
       "      <td>0.753073</td>\n",
       "      <td>460734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>182.089996</td>\n",
       "      <td>185.600006</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>185.559998</td>\n",
       "      <td>185.559998</td>\n",
       "      <td>59144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>183.919998</td>\n",
       "      <td>185.149994</td>\n",
       "      <td>182.729996</td>\n",
       "      <td>185.139999</td>\n",
       "      <td>185.139999</td>\n",
       "      <td>42841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>184.350006</td>\n",
       "      <td>186.399994</td>\n",
       "      <td>183.919998</td>\n",
       "      <td>186.190002</td>\n",
       "      <td>186.190002</td>\n",
       "      <td>46792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>186.539993</td>\n",
       "      <td>187.050003</td>\n",
       "      <td>183.619995</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>49128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>186.059998</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>185.190002</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>40444700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6046 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2000-01-03    0.936384    1.004464    0.907924    0.999442    0.847207   \n",
       "1     2000-01-04    0.966518    0.987723    0.903460    0.915179    0.775779   \n",
       "2     2000-01-05    0.926339    0.987165    0.919643    0.928571    0.787131   \n",
       "3     2000-01-06    0.947545    0.955357    0.848214    0.848214    0.719014   \n",
       "4     2000-01-07    0.861607    0.901786    0.852679    0.888393    0.753073   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6041  2024-01-08  182.089996  185.600006  181.500000  185.559998  185.559998   \n",
       "6042  2024-01-09  183.919998  185.149994  182.729996  185.139999  185.139999   \n",
       "6043  2024-01-10  184.350006  186.399994  183.919998  186.190002  186.190002   \n",
       "6044  2024-01-11  186.539993  187.050003  183.619995  185.589996  185.589996   \n",
       "6045  2024-01-12  186.059998  186.740005  185.190002  185.919998  185.919998   \n",
       "\n",
       "         Volume  \n",
       "0     535796800  \n",
       "1     512377600  \n",
       "2     778321600  \n",
       "3     767972800  \n",
       "4     460734400  \n",
       "...         ...  \n",
       "6041   59144500  \n",
       "6042   42841800  \n",
       "6043   46792900  \n",
       "6044   49128400  \n",
       "6045   40444700  \n",
       "\n",
       "[6046 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock = pd.read_csv(r\"C:\\Users\\ASatl\\OneDrive\\Desktop\\let\\AAPL.csv\")\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Word2Vec embeddings\n",
    "tweet_embeddings = []\n",
    "for tweet_text in tweet_texts:\n",
    "    tweet_embedding = [word2vec_model.wv[word] for word in tweet_text.split() if word in word2vec_model.wv]\n",
    "    if tweet_embedding:\n",
    "        tweet_embedding_avg = np.mean(tweet_embedding, axis=0)\n",
    "        tweet_embeddings.append(tweet_embedding_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate features and target\n",
    "X = np.array(tweet_embeddings)\n",
    "y = df_stock['Close'].values[:len(tweet_embeddings)]  # Use only as many stock prices as the number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP Regressor model\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# Train MLP Regressor model\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict stock prices\n",
    "predictions = mlp_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.01515188934633999\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
